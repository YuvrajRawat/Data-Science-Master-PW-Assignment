{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. An activation function in the context of artificial neural networks is a mathematical function that determines the output of\n",
    "a neuron. It takes the weighted sum of inputs and produces an output that is then passed to the next layer of neurons.\n",
    "\n",
    "Q2. Some common types of activation functions used in neural networks include:\n",
    "\n",
    "Sigmoid\n",
    "Hyperbolic tangent (tanh)\n",
    "Rectified Linear Unit (ReLU)\n",
    "Leaky ReLU\n",
    "Softmax\n",
    "Q3. Activation functions affect the training process and performance of a neural network by introducing non-linearity into the \n",
    "model, allowing it to learn complex patterns in the data. They also influence the convergence speed during training and can help\n",
    "mitigate issues such as vanishing or exploding gradients.\n",
    "\n",
    "Q4. The sigmoid activation function squashes the input values between 0 and 1. It works by mapping any input value to a value \n",
    "between 0 and 1. Its advantages include smooth gradient, which enables efficient learning using gradient descent. However, it \n",
    "suffers from the vanishing gradient problem, where gradients become extremely small, hindering learning, especially in deep \n",
    "networks.\n",
    "\n",
    "Q5. The Rectified Linear Unit (ReLU) activation function returns 0 for negative inputs and the input value for positive inputs. \n",
    "It differs from the sigmoid function in that it is not bounded, allowing for faster and more efficient training, especially in \n",
    "deep neural networks.\n",
    "\n",
    "Q6. The benefits of using the ReLU activation function over the sigmoid function include faster convergence during training, \n",
    "reduced likelihood of the vanishing gradient problem, and simplicity in computation.\n",
    "\n",
    "Q7. Leaky ReLU is a variation of the ReLU activation function that allows a small, positive gradient when the input is negative,\n",
    "rather than setting it to zero. This addresses the vanishing gradient problem by providing a non-zero gradient for negative \n",
    "inputs, allowing for better learning in deep neural networks.\n",
    "\n",
    "Q8. The softmax activation function is commonly used in the output layer of neural networks for multi-class classification \n",
    "problems. It converts raw scores or logits into probabilities, with each output representing the likelihood of the corresponding\n",
    "class.\n",
    "\n",
    "Q9. The hyperbolic tangent (tanh) activation function is similar to the sigmoid function but squashes the input values between \n",
    "-1 and 1. It compares to the sigmoid function in that it suffers from the vanishing gradient problem, but it has the advantage \n",
    "of outputting zero-centered values, which can aid in learning dynamics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
