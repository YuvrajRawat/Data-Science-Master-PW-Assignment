{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Understanding Pooling and Padding in CNN\n",
    "    \n",
    "1. Pooling in CNN (Convolutional Neural Networks):\n",
    "\n",
    "Purpose: The purpose of pooling in CNN is to downsample the spatial dimensions (width and height) of feature maps while \n",
    "    retaining important information. Pooling helps reduce the computational complexity of the network, makes the model more \n",
    "    robust to small variations in input, and helps prevent overfitting by introducing a degree of translation invariance.\n",
    "Benefits:\n",
    "Dimensionality Reduction: Pooling reduces the size of feature maps, which reduces the number of parameters and computations in \n",
    "    subsequent layers.\n",
    "Translation Invariance: Pooling helps the network recognize patterns regardless of their precise location in the input.\n",
    "Feature Selection: By selecting the most relevant information from a local neighborhood, pooling focuses on essential features \n",
    "    while discarding less important ones.\n",
    "\n",
    "2. Difference between Min Pooling and Max Pooling:\n",
    "\n",
    "Max Pooling: In max pooling, for each pooling region (e.g., a 2x2 window), the maximum value within that region is selected as \n",
    "    the output value. It is often used to highlight the most important feature in a region.\n",
    "Min Pooling: Min pooling is similar to max pooling, but it selects the minimum value within the pooling region instead of the \n",
    "    maximum. This can be useful in some applications where the presence of specific features is indicated by low values.\n",
    "\n",
    "3. Padding in CNN:\n",
    "\n",
    "Concept: Padding in CNN involves adding extra border pixels (usually zeros) around the input image or feature map before \n",
    "    applying convolution or pooling operations. Padding is used to control the spatial dimensions of the output feature maps.\n",
    "Significance: Padding is essential for several reasons:\n",
    "Preserving Spatial Dimensions: Padding ensures that the spatial dimensions of the output feature maps are the same or similar \n",
    "    to the input's spatial dimensions.\n",
    "Edge Information: Without padding, the information at the edges of the input may be underrepresented in the output feature maps.\n",
    "Control over Convolution Size: Padding allows us to control the size of the convolutional output.\n",
    "\n",
    "4. Zero-padding vs. Valid-padding:\n",
    "\n",
    "Zero-padding: Zero-padding adds zeros around the input feature map, increasing its spatial dimensions. It is often used when you\n",
    "    want to maintain the spatial dimensions of the input in the output feature maps or when you need to apply convolutional \n",
    "    layers without reducing the spatial size.\n",
    "Valid-padding: Valid-padding (also known as no padding) does not add any extra pixels around the input. It reduces the spatial \n",
    "    dimensions of the output feature maps compared to the input. This is commonly used when you want to reduce the spatial \n",
    "    dimensions, which typically occurs in later layers of a CNN.\n",
    "In summary, zero-padding maintains spatial dimensions, while valid-padding reduces them. The choice between these padding types \n",
    "depends on the specific architectural requirements of the CNN and the desired output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd78d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Exploring LeNet\n",
    "    \n",
    "1. Overview of LeNet-5 Architecture:\n",
    "LeNet-5 is a convolutional neural network (CNN) architecture developed by Yann LeCun and his colleagues in the 1990s. It is one \n",
    "of the pioneering CNN architectures and was designed for handwritten digit recognition, making it particularly well-suited for \n",
    "image classification tasks. LeNet-5 played a crucial role in the development of modern CNNs.\n",
    "\n",
    "2. Key Components of LeNet-5:\n",
    "LeNet-5 consists of the following key components:\n",
    "\n",
    "Input Layer: Accepts grayscale images with a fixed size (usually 32x32 or 28x28 pixels).\n",
    "\n",
    "Convolutional Layers: LeNet-5 has two sets of convolutional layers:\n",
    "\n",
    "The first convolutional layer applies a 5x5 filter with a ReLU activation function.\n",
    "The second convolutional layer applies a 5x5 filter with a ReLU activation.\n",
    "Both layers are followed by 2x2 max-pooling layers.\n",
    "Fully Connected Layers: After the convolutional layers, LeNet-5 has three fully connected layers:\n",
    "\n",
    "The first fully connected layer has 120 neurons with a ReLU activation.\n",
    "The second fully connected layer has 84 neurons with a ReLU activation.\n",
    "The final output layer has the number of neurons equal to the number of classes in the dataset.\n",
    "Activation Functions: LeNet-5 primarily uses the ReLU activation function, which introduces non-linearity into the network.\n",
    "\n",
    "Pooling Layers: LeNet-5 employs max-pooling layers after the convolutional layers, helping to downsample feature maps and reduce\n",
    "    computational complexity.\n",
    "\n",
    "3. Advantages and Limitations of LeNet-5:\n",
    "\n",
    "Advantages:\n",
    "LeNet-5 was groundbreaking at its time and demonstrated the effectiveness of CNNs for image classification.\n",
    "It introduced the concept of convolutional layers and weight sharing, which are fundamental to modern CNN architectures.\n",
    "Well-suited for simple image classification tasks, such as handwritten digit recognition.\n",
    "Limitations:\n",
    "Limited capacity: LeNet-5 may not perform well on more complex tasks with larger and diverse datasets.\n",
    "Shallow architecture: Compared to modern CNNs, it has a relatively shallow architecture, which may not capture intricate \n",
    "    features in complex images.\n",
    "Not suitable for large, high-resolution images due to its fixed input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5ea567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.  Implementing and Training LeNet-5:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "054b8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07aa4694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 9912422/9912422 [00:02<00:00, 3890781.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 4123768.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 2654583.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e573ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LeNet-5 model, loss function, and optimizer\n",
    "model = LeNet5()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf68063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.26368189649904633\n",
      "Epoch 2, Loss: 0.06924514895774671\n",
      "Epoch 3, Loss: 0.04824028703518637\n",
      "Epoch 4, Loss: 0.03878923371809769\n",
      "Epoch 5, Loss: 0.03059123864248226\n",
      "Epoch 6, Loss: 0.028147415411266398\n",
      "Epoch 7, Loss: 0.02500297809285591\n",
      "Epoch 8, Loss: 0.0213476820416017\n",
      "Epoch 9, Loss: 0.018093869924655094\n",
      "Epoch 10, Loss: 0.016143703104969234\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0cda52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test dataset\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22ebde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.93%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
