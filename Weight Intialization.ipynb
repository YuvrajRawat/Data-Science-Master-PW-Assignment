{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffdca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 1: Understanding Weight Initialization\n",
    "\n",
    "1. Importance of Weight Initialization:\n",
    "Weight initialization is a critical step in training artificial neural networks (ANNs) because it influences the learning \n",
    "process and overall performance of the network. The primary reasons for careful weight initialization are:\n",
    "\n",
    "Avoiding Symmetry: Without proper initialization, all neurons in a layer might start with the same weights, leading to symmetry \n",
    "    in the network. This symmetry makes all neurons in a layer learn the same features and does not allow the network to capture\n",
    "    diverse information.\n",
    "\n",
    "Speeding up Convergence: Proper initialization helps the network converge faster during training. If weights are initialized too\n",
    "    large, gradients can be too high, causing oscillations or divergence. If weights are initialized too small, gradients can be\n",
    "    vanishing, making learning extremely slow.\n",
    "\n",
    "Preventing Vanishing and Exploding Gradients: In deep networks, improper weight initialization can lead to the vanishing or \n",
    "    exploding gradient problem. This can make it extremely difficult for the network to update its weights effectively, \n",
    "    especially in deep architectures.\n",
    "\n",
    "2. Challenges with Improper Weight Initialization:\n",
    "Improper weight initialization can lead to several issues during model training and convergence:\n",
    "\n",
    "Vanishing Gradients: When weights are initialized too small, during backpropagation, the gradients can become vanishingly small \n",
    "    as they are propagated backward through the layers. This can hinder learning in deep networks and lead to slow convergence.\n",
    "\n",
    "Exploding Gradients: Conversely, when weights are initialized too large, gradients can explode during backpropagation, causing \n",
    "    the optimization process to become unstable and diverge.\n",
    "\n",
    "Convergence to Poor Local Minima: Poor initialization can lead the optimization algorithm to converge to suboptimal local \n",
    "    minima rather than finding the global minimum of the loss function.\n",
    "\n",
    "Long Training Times: Inefficient initialization can result in very long training times, making the model impractical to use.\n",
    "\n",
    "3. Variance and Weight Initialization:\n",
    "Variance refers to the measure of how much the values in a set of data differ from the mean. In the context of weight \n",
    "initialization, variance plays a crucial role:\n",
    "\n",
    "Weight initialization methods typically involve setting the initial values of weights from a certain distribution, such as a \n",
    "Gaussian or uniform distribution. The variance of this distribution determines the spread of initial weight values.\n",
    "\n",
    "Properly chosen variance ensures that the initial weights are neither too small (vanishing gradients) nor too large (exploding \n",
    "gradients). It helps in balancing the scale of the activations and gradients throughout the network, which is essential for \n",
    "stable and efficient training.\n",
    "\n",
    "Variance in weight initialization methods needs to be carefully tuned to match the activation functions, network architecture, \n",
    "and the scale of the problem, and different initialization techniques aim to strike the right balance to address these \n",
    "considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Weight lnitialization Techniques\n",
    "\n",
    "1. Zero Initialization:\n",
    "\n",
    "Concept: Zero initialization involves setting all the weights and biases in a neural network to zero initially. This means that \n",
    "    every neuron in the network will have the same weights and biases, leading to symmetry in learning. This approach seems \n",
    "    intuitive, as it starts with a neutral position where the network doesn't have any prior information. However, it comes \n",
    "    with significant limitations.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Symmetry Problem: If all the weights are initialized to zero, all neurons in a layer will have the same gradient during \n",
    "    backpropagation. Consequently, they will learn the same features and essentially behave like one neuron. This makes it \n",
    "    impossible for the network to learn complex patterns and slows down convergence.\n",
    "Vanishing Gradients: The network is prone to vanishing gradients, especially when using activation functions like sigmoid or \n",
    "    hyperbolic tangent (tanh). These functions have derivatives close to zero around zero, so gradients become extremely small \n",
    "    during backpropagation, making weight updates negligible.\n",
    "Appropriate Use: Zero initialization is rarely used in practice due to its limitations. It can be suitable for specific \n",
    "    situations, such as initializing bias terms in some layers or for toy problems, but it is generally avoided for weight \n",
    "    initialization in deep neural networks.\n",
    "\n",
    "2. Random Initialization:\n",
    "\n",
    "Concept: Random initialization is a commonly used technique where weights and biases are initialized with random values drawn \n",
    "    from a specified distribution. This helps break the symmetry in the network and prevents neurons from learning the same \n",
    "    features.\n",
    "\n",
    "Adjustments to Mitigate Issues:\n",
    "\n",
    "Gaussian Initialization: You can initialize weights using a Gaussian (normal) distribution with a mean of 0 and a small standard\n",
    "    deviation (e.g., 0.01). This can help avoid vanishing/exploding gradients.\n",
    "Xavier/Glorot Initialization: This method uses a Gaussian distribution with a mean of 0 and a specific variance that depends on\n",
    "    the number of input and output units. It helps address the vanishing/exploding gradients problem by maintaining reasonable\n",
    "    variance in activations.\n",
    "\n",
    "3. Xavier/Glorot Initialization:\n",
    "\n",
    "Concept: Xavier (or Glorot) initialization is a specific type of random initialization designed to address the problems of \n",
    "    improper weight initialization. It sets the variance of the initial weights in a layer to a value that depends on the number\n",
    "    of input and output units. The idea is to maintain a balanced variance, which is crucial for training deep networks.\n",
    "\n",
    "Underlying Theory: The variance of Xavier initialization is calculated as 2 / (n_in + n_out), where n_in is the number of input \n",
    "    units and n_out is the number of output units. This initialization ensures that the weights have reasonable values to avoid \n",
    "    vanishing or exploding gradients during training. It is particularly effective when using activation functions like the \n",
    "    hyperbolic tangent (tanh) or sigmoid.\n",
    "\n",
    "4. He Initialization:\n",
    "\n",
    "Concept: He initialization is another random weight initialization technique, but it differs from Xavier initialization. In He \n",
    "    initialization, the variance of the initial weights is calculated as 2 / n_in, where n_in is the number of input units. This\n",
    "    initialization is suited for activation functions like the Rectified Linear Unit (ReLU).\n",
    "\n",
    "Differences from Xavier: He initialization is designed to work well with the ReLU activation function, which has a non-zero \n",
    "    derivative for positive values. The variance in He initialization is higher than in Xavier, making it more suitable for ReLU\n",
    "    as it avoids the vanishing gradient problem.\n",
    "\n",
    "Preferred Use: He initialization is commonly preferred when ReLU and its variants are used as activation functions in deep \n",
    "    neural networks. It helps maintain appropriate weight values and facilitates efficient training by preventing vanishing \n",
    "    gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98cff6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Applying Weight Initialization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94146c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb42ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset (e.g., MNIST)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "640277b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, initialization, num_epochs=10):\n",
    "    if initialization == 'xavier':\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "    elif initialization == 'he':\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.kaiming_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.view(inputs.shape[0], -1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b32507cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different weight initializations\n",
    "input_size = 28 * 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Zero initialization\n",
    "train(model, initialization='zero')\n",
    "# Training code and evaluation for the zero initialization case\n",
    "\n",
    "# Random initialization\n",
    "train(model, initialization='random')\n",
    "# Training code and evaluation for the random initialization case\n",
    "\n",
    "# Xavier initialization\n",
    "train(model, initialization='xavier')\n",
    "# Training code and evaluation for the Xavier initialization case\n",
    "\n",
    "# He initialization\n",
    "train(model, initialization='he')\n",
    "# Training code and evaluation for the He initialization case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
