{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ce375",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of batch normalization in the context of Artificial Neural Networks:\n",
    "\n",
    "Batch normalization is a technique used to improve the training stability and performance of artificial neural networks. It \n",
    "involves normalizing the activations of each layer by adjusting and scaling them so they have a standard mean and variance. \n",
    "This normalization helps in reducing internal covariate shift, which is the change in the distribution of network activations \n",
    "due to parameter updates during training. By stabilizing the distribution of inputs to each layer, batch normalization allows \n",
    "for faster and more stable training of deep neural networks.\n",
    "\n",
    "Q2. Describe the benefits of using batch normalization during training:\n",
    "\n",
    "The benefits of using batch normalization during training include:\n",
    "\n",
    "Improved convergence: Batch normalization helps in stabilizing the training process by maintaining a consistent distribution of \n",
    "    inputs, which leads to faster convergence.\n",
    "Regularization: Batch normalization acts as a form of regularization, reducing the risk of overfitting by adding noise to the \n",
    "    activations during training.\n",
    "Reduced sensitivity to initialization: Batch normalization reduces the dependency of the network on the initial weights, making \n",
    "    it less sensitive to weight initialization schemes.\n",
    "Gradient flow: Batch normalization helps in mitigating the vanishing gradient problem by ensuring that gradients flow smoothly \n",
    "    during backpropagation.\n",
    "Q3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters:\n",
    "\n",
    "The working principle of batch normalization involves two main steps:\n",
    "\n",
    "Normalization: For each mini-batch of training data, batch normalization normalizes the activations of each layer by subtracting\n",
    "    the mean and dividing by the standard deviation of the batch. This step ensures that the activations have a standard mean \n",
    "    and variance, which helps in stabilizing the training process.\n",
    "Learnable parameters: Batch normalization introduces two learnable parameters, usually denoted as gamma (γ) and beta (β), for \n",
    "    each layer. These parameters allow the network to adaptively scale and shift the normalized activations, giving it more \n",
    "    flexibility during training.\n",
    "Q4. Implementation:\n",
    "\n",
    "To implement batch normalization, you can use any deep learning framework/library like TensorFlow or PyTorch. Here's a basic \n",
    "outline of the implementation process:\n",
    "\n",
    "Choose a dataset (e.g., MNIST, CIFAR-10) and preprocess it.\n",
    "Implement a simple feedforward neural network without using batch normalization.\n",
    "Train the neural network on the chosen dataset.\n",
    "Implement batch normalization layers in the neural network and train the model again.\n",
    "Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization.\n",
    "Q5. Experimentation and Analysis:\n",
    "\n",
    "Experiment with different batch sizes and observe the effect on the training dynamics and model performance. Discuss the \n",
    "advantages and potential limitations of batch normalization in improving the training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1188208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 784  # 28x28\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Feedforward neural network without batch normalization\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Feedforward neural network with batch normalization\n",
    "class FeedForwardNN_BN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNN_BN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)  # Batch normalization layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize models\n",
    "model_without_bn = FeedForwardNN(input_size, hidden_size, num_classes)\n",
    "model_with_bn = FeedForwardNN_BN(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_without_bn = optim.Adam(model_without_bn.parameters(), lr=learning_rate)\n",
    "optimizer_with_bn = optim.Adam(model_with_bn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model without batch normalization\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_without_bn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer_without_bn.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_without_bn.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Testing the model without batch normalization\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model_without_bn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network without batch normalization on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Training the model with batch normalization\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_with_bn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer_with_bn.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_with_bn.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Testing the model with batch normalization\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model_with_bn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network with batch normalization on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
